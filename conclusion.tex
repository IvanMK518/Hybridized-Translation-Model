\section{Conclusion}

This study explores low-resource language processing and dialect adaptation in Haitian Creole, focusing on combining Linguistically Diverse Prompting (LDP) and Polyglot Prompting techniques with contextual markers. Our Hybrid Model achieved an F1 score of 0.924 in Machine Reading Comprehension and a BLEU score of 18.057 in Machine Translation in Haitian Creole. While the BLEU score indicates room for improvement, particularly with similar low-resource languages, our findings show that complex models excel in comprehension and translation but under-perform in Natural Language Inference tasks, where simpler models are more consistent.

We introduced a novel approach building on LDP and Polyglot Prompting to adapt models for dialectal variations, analyzing AAVE and Haitian Creole. Error analysis revealed challenges with linguistic ambiguity and code-switching in the tagging, providing insights on improving linguistic and dialectal tagging.

Future work includes expanding to other low-resource languages tested in other studies \cite{Faisal:24} \cite{Nguyen:24} \cite{Ng:22}, observing cross-lingual tasks with audio datasets for low-resource languages, refining evaluation metrics for translation quality, and fine-tuning models to enhance Natural Language Inference performance while maintaining translation and comprehension quality. By testing the broader applicability of our hybridization technique, we can expand the inclusion of other under-developed countries using LLMs.