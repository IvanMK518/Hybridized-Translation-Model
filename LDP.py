# -*- coding: utf-8 -*-
"""LDP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rwBIUcvhbrigwx_7K_bzQ02H0s9R-k0X
"""

# Cell 1: Initial Setup and Installations
!pip install openai
!pip install gdown
!pip uninstall -y openai
!pip install openai==0.28.0
!pip install spacy transformers sacrebleu bert-score torch
!python -m spacy download en_core_web_lg

# Cell 2: Imports and Initializations
import openai
import gdown
import json
import pandas as pd
import spacy
from transformers import pipeline
from sacrebleu.metrics import BLEU
from bert_score import score as bert_score
import numpy as np
from typing import List, Dict, Any
import time
from json.decoder import JSONDecodeError
from google.colab import drive

# Initialize NLP components
nlp = spacy.load("en_core_web_lg")
nli_pipeline = pipeline("zero-shot-classification")
qa_pipeline = pipeline("question-answering")

# Initialize API key
openai.api_key = "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"

# Cell 3: Evaluation Metrics Class
class EvaluationMetrics:
    @staticmethod
    def evaluate_dialect(text: str) -> Dict[str, float]:
        doc = nlp(text)
        features = {
            'sentence_length': len(text.split()),
            'unique_words': len(set(text.lower().split())),
            'pos_distribution': dict(pd.Series([token.pos_ for token in doc]).value_counts(normalize=True))
        }
        return features

    @staticmethod
    def evaluate_ner(text: str) -> Dict[str, List[Dict]]:
        doc = nlp(text)
        entities = [{'text': ent.text, 'label': ent.label_} for ent in doc.ents]
        return {'entities': entities, 'count': len(entities)}

    @staticmethod
    def evaluate_nli(premise: str, hypothesis: str) -> Dict[str, float]:
        try:
            result = nli_pipeline(
                sequences=hypothesis,
                candidate_labels=["entailment", "contradiction", "neutral"],
            )
            return {
                'label': result['labels'][0],
                'score': result['scores'][0]
            }
        except Exception as e:
            print(f"NLI evaluation error: {e}")
            return {
                'label': 'error',
                'score': 0.0
            }

    @staticmethod
    def evaluate_comprehension(context: str, response: str) -> Dict[str, float]:
        try:
            relevance_score = nlp(context).similarity(nlp(response))
            return {'relevance': float(relevance_score)}
        except Exception as e:
            print(f"Comprehension evaluation error: {e}")
            return {'relevance': 0.0}

    @staticmethod
    def evaluate_topic(text: str) -> Dict[str, str]:
        try:
            result = nli_pipeline(
                sequences=text,
                candidate_labels=["translation", "cultural_analysis", "dialogue", "linguistic"]
            )
            return {
                'predicted_topic': result['labels'][0],
                'confidence': float(result['scores'][0])
            }
        except Exception as e:
            print(f"Topic evaluation error: {e}")
            return {
                'predicted_topic': 'error',
                'confidence': 0.0
            }

    @staticmethod
    def evaluate_translation(source: str, target: str) -> Dict[str, float]:
        try:
            bleu = BLEU()
            bleu_score = bleu.corpus_score([target], [[source]]).score
            return {'bleu_score': float(bleu_score)}
        except Exception as e:
            print(f"Translation evaluation error: {e}")
            return {'bleu_score': 0.0}

# Cell 4: Helper Functions
def download_from_drive(file_id, output_name):
    gdown.download(f"https://drive.google.com/uc?id={file_id}", output_name, quiet=False)

def get_gpt4o_response(prompt: str) -> Dict[str, Any]:
    try:
        response = openai.ChatCompletion.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=2500,
            temperature=0.7
        )
        response_text = response['choices'][0]['message']['content'].strip()

        # Add evaluations
        evaluations = {}
        try:
            evaluations['dialect_features'] = EvaluationMetrics.evaluate_dialect(response_text)
            evaluations['named_entities'] = EvaluationMetrics.evaluate_ner(response_text)
            evaluations['nli_scores'] = EvaluationMetrics.evaluate_nli(prompt, response_text)
            evaluations['comprehension'] = EvaluationMetrics.evaluate_comprehension(prompt, response_text)
            evaluations['topic'] = EvaluationMetrics.evaluate_topic(response_text)
            evaluations['translation_quality'] = EvaluationMetrics.evaluate_translation(prompt, response_text)
        except Exception as e:
            print(f"Evaluation error: {e}")
            evaluations['error'] = str(e)

        return {
            'response': response_text,
            'evaluations': evaluations
        }
    except Exception as e:
        print(f"GPT-4o response error: {e}")
        return {'error': str(e)}

def create_bin_prompt(bin_D2):
    prompt = "You are provided with a set of data entries from Dataset D2:\n\n"

    for idx, entry_D2 in enumerate(bin_D2, start=1):
        # Extract specified D2 fields
        context = entry_D2.get('Context', '')
        response = entry_D2.get('Response', '')
        rots = entry_D2.get('ROTS', [])
        safety_annotations = entry_D2.get('Safety Annotations', [])
        tags_D2 = entry_D2.get('Tags', {})

        prompt += (
            f"**Entry {idx} from Dataset D2**:\n"
            f"Context: {context}\nResponse: {response}\n"
            f"ROTS: {rots}\nSafety Annotations: {safety_annotations}\nTags: {tags_D2}\n\n"
        )

    prompt += """**Task**:
    1. Observe the linguistic features in each entry.
    2. Translate the entry to Haitian Creole, and perform a linguistic analysis on the translation.
    3. Provide the cultural and conversational context of the translation.
    4. Provide a dialogue response in Haitian Creole.
    Note: Carry out this set of tasks once per bin. It is important to analyze the full contents of the bin to provide a proper analysis.
    Begin."""

    return prompt

# Cell 5: Data Loading and Processing
# Download and load D2 dataset
download_from_drive("1V9mtYGv4XWfvPH6sUk82T6QbBBdcGsuh", "D2_dataset.json")

with open("D2_dataset.json", "r", encoding="utf-8") as file:
    data_D2 = json.load(file)

df_D2 = pd.json_normalize(data_D2)

# Preview the dataset
print("D2 Sample:")
display(df_D2.head())
print("D2 Sample (last 5 rows):")
display(df_D2.tail())

# Cell 6: Main Processing Loop
outputs = []
max_retries = 3
retry_delay = 5
bin_size = 20

num_bins = len(df_D2) // bin_size

for bin_index in range(num_bins):
    bin_D2 = df_D2.iloc[bin_index * bin_size : (bin_index + 1) * bin_size].to_dict(orient="records")
    prompt = create_bin_prompt(bin_D2)
    print(f"Processing bin {bin_index + 1}/{num_bins}")

    for attempt in range(max_retries):
        try:
            result = get_gpt4o_response(prompt)
            outputs.append({
                'Bin': bin_index + 1,
                'Prompt': prompt,
                'Response': result['response'],
                'Evaluations': result.get('evaluations', {})
            })
            break
        except (openai.error.APIError, JSONDecodeError) as e:
            print(f"Error on bin {bin_index + 1}, attempt {attempt + 1}/{max_retries}: {e}")
            if attempt < max_retries - 1:
                time.sleep(retry_delay)
            else:
                print(f"Failed to process bin {bin_index + 1} after {max_retries} attempts.")

from google.colab import drive
from google.colab import files
import matplotlib.pyplot as plt
import seaborn as sns


# Cell 7: Save, Display, and Download Results
# Mount Google Drive
drive.mount('/content/drive')

# Create DataFrame with outputs
df_outputs = pd.DataFrame(outputs)


# 2. Save locally and download
local_csv_path = "gptbase_evaluated_outputs.csv"
df_outputs.to_csv(local_csv_path, index=False)
files.download(local_csv_path)

# Create evaluation summary
print("\nEvaluation Metrics Summary:")
evaluation_metrics = pd.json_normalize(df_outputs['Evaluations'].tolist())
display(evaluation_metrics.describe())

# Create visualizations
plt.figure(figsize=(15, 10))

plt.subplot(2, 2, 1)
sns.lineplot(data=evaluation_metrics, x=evaluation_metrics.index, y='translation_quality.bleu_score')
plt.title('BLEU Scores by Entry')

plt.subplot(2, 2, 2)
sns.lineplot(data=evaluation_metrics, x=evaluation_metrics.index, y='comprehension.relevance')
plt.title('Comprehension Scores by Entry')

plt.subplot(2, 2, 3)
sns.lineplot(data=evaluation_metrics, x=evaluation_metrics.index, y='nli_scores.score')
plt.title('NLI Scores by Entry')

plt.subplot(2, 2, 4)
sns.lineplot(data=evaluation_metrics, x=evaluation_metrics.index, y='named_entities.count')
plt.title('Named Entity Count by Entry')

plt.tight_layout()

# Save visualization to both Google Drive and locally for download
gdrive_viz_path = '/content/drive/My Drive/Results/evaluation_metrics.png'
local_viz_path = 'evaluation_metrics.png'

# Save to Google Drive
plt.savefig(gdrive_viz_path)
print(f"Visualization saved to Google Drive at: {gdrive_viz_path}")

# Save locally and download
plt.savefig(local_viz_path)
files.download(local_viz_path)

# Save evaluation metrics separately
evaluation_metrics.to_csv('evaluation_metrics.csv', index=False)
files.download('evaluation_metrics.csv')
