# -*- coding: utf-8 -*-
"""GPTbasemodel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SI499rS0AUzJD50OusDOfNYZ8fnBFCOK
"""

!pip install openai
!pip install gdown
!pip uninstall -y openai
!pip install openai==0.28.0

import openai
import gdown
import json
import pandas as pd

openai.api_key = "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"

# Function to download files from Google Drive
def download_from_drive(file_id, output_name):
    gdown.download(f"https://drive.google.com/uc?id={file_id}", output_name, quiet=False)

# Download new D1 and D2
download_from_drive("1V9mtYGv4XWfvPH6sUk82T6QbBBdcGsuh", "D2_dataset.json")

# Load datasets
with open("D2_dataset.json", "r", encoding="utf-8") as file:
    data_D2 = json.load(file)

# Convert data to pandas DataFrames for easier manipulation
df_D2 = pd.json_normalize(data_D2)

# Preview the datasets
print("D2 Sample:")
display(df_D2.head())


print("D2 Sample (last 5 rows):")
display(df_D2.tail())

import openai

bin_size = 20

# Function to create a prompt for a bin of entries with specified D2 field extraction
def create_bin_prompt(bin_D2):
    prompt = "You are provided with a set of data entries from Dataset D2:\n\n"

    for idx, entry_D2 in enumerate(bin_D2, start=1):
        # Extract specified D2 fields
        context = entry_D2.get('Context', '')
        response = entry_D2.get('Response', '')
        rots = entry_D2.get('ROTS', [])
        safety_annotations = entry_D2.get('Safety Annotations', [])
        tags_D2 = entry_D2.get('Tags', {})

        # Append each entry as part of the bin in the prompt
        prompt += (
            f"**Entry {idx} from Dataset D2**:\n"
            f"Context: {context}\nResponse: {response}\n"
            f"ROTS: {rots}\nSafety Annotations: {safety_annotations}\nTags: {tags_D2}\n\n"
        )

    # Task Instructions
    prompt += (
        "**Task**:\n"
        "1. Observe the linguistic features in each entry.\n"
        "2. Translate the entry to Haitian Creole, and perform a linguistic analysis on the translation.\n"
        "3. Provide the cultural and conversational context of the translation.\n"
        "4. Provide a dialogue response in Haitian Creole.\n"
        "Note: Carry out this set of tasks once per bin. It is important to analyze the full contents of the bin to provide a proper analysis.\n"
        "Begin."
    )

    return prompt

import openai

def get_gpt4o_response(prompt):
    response = openai.ChatCompletion.create(
        model="gpt-4o",  # Ensure "gpt-4o" is the correct model identifier for your use case
        messages=[{"role": "user", "content": prompt}],
        max_tokens=2500,
        temperature=0.7
    )
    return response['choices'][0]['message']['content'].strip()

import time
import openai

# List to store outputs
outputs = []

# Retry parameters
max_retries = 3
retry_delay = 5  # seconds

# Calculate the number of bins
num_bins = len(df_D2) // bin_size

for bin_index in range(num_bins):
    # Select the entries for the current bin and convert them to dictionaries
    bin_D2 = df_D2.iloc[bin_index * bin_size : (bin_index + 1) * bin_size].to_dict(orient="records")

    # Create prompt for the current bin using only D2
    prompt = create_bin_prompt(bin_D2)
    print(f"Processing bin {bin_index + 1}/{num_bins}")

    # Retry loop for API call
    for attempt in range(max_retries):
        try:
            # Get GPT-4o response for the bin
            response = get_gpt4o_response(prompt)

            # Store the result if successful
            outputs.append({
                'Bin': bin_index + 1,
                'Prompt': prompt,
                'Response': response
            })
            break  # Exit retry loop if successful

        except openai.error.APIError as e:
            print(f"API error on bin {bin_index + 1}, attempt {attempt + 1}/{max_retries}: {e}")
            if attempt < max_retries - 1:
                time.sleep(retry_delay)  # Wait before retrying
            else:
                print(f"Failed to process bin {bin_index + 1} after {max_retries} attempts.")

        except JSONDecodeError as e:
            print(f"JSON decode error on bin {bin_index + 1}, attempt {attempt + 1}/{max_retries}: {e}")
            if attempt < max_retries - 1:
                time.sleep(retry_delay)  # Wait before retrying
            else:
                print(f"Failed to process bin {bin_index + 1} after {max_retries} attempts.")

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

# Assuming `outputs` is your list of results
df_outputs = pd.DataFrame(outputs)

# Define the file path in the 'Results' folder in your Google Drive
file_path = '/content/drive/My Drive/Results/gptbase_outputs.csv'

# Save the DataFrame to the specified path
df_outputs.to_csv(file_path, index=False)

print(f"File saved to {file_path}")