# -*- coding: utf-8 -*-
"""ComboModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i43VEOrmvH0xZlPKF_psfoXd63LvId18
"""

# Install required packages
!pip install openai==0.28.0 gdown spacy transformers sacrebleu bert-score torch
!python -m spacy download en_core_web_lg

import openai
import gdown
import json
import pandas as pd
import spacy
from transformers import pipeline
from sacrebleu.metrics import BLEU
from bert_score import score as bert_score
import numpy as np
from typing import List, Dict, Any
from google.colab import files
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize API key
openai.api_key = "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"

# Initialize NLP components
nlp = spacy.load("en_core_web_lg")
nli_pipeline = pipeline("zero-shot-classification")
qa_pipeline = pipeline("question-answering")

# Function to download files from Google Drive
def download_from_drive(file_id, output_name):
    gdown.download(f"https://drive.google.com/uc?id={file_id}", output_name, quiet=False)

# Download datasets
download_from_drive("10P4Vvombs_lsFjTwEO5rskpEu6RP8MnL", "D1_dataset.json")
download_from_drive("1V9mtYGv4XWfvPH6sUk82T6QbBBdcGsuh", "D2_dataset.json")
download_from_drive("10A56TDD5V4kh9C-oKB75F4_2TdLp4yq-", "D3_dataset.json")

# Load datasets
with open("D1_dataset.json", "r", encoding="utf-8") as file:
    data_D1 = json.load(file)
with open("D2_dataset.json", "r", encoding="utf-8") as file:
    data_D2 = json.load(file)
with open("D3_dataset.json", "r", encoding="utf-8") as file:
    data_D3 = json.load(file)

# Convert to DataFrames
df_D1 = pd.json_normalize(data_D1)
df_D2 = pd.json_normalize(data_D2)
df_D3 = pd.json_normalize(data_D3)

class EvaluationMetrics:
    @staticmethod
    def evaluate_dialect(text: str) -> Dict[str, float]:
        doc = nlp(text)
        features = {
            'sentence_length': len(text.split()),
            'unique_words': len(set(text.lower().split())),
            'pos_distribution': dict(pd.Series([token.pos_ for token in doc]).value_counts(normalize=True))
        }
        return features

    @staticmethod
    def evaluate_ner(text: str) -> Dict[str, List[Dict]]:
        doc = nlp(text)
        entities = [{'text': ent.text, 'label': ent.label_} for ent in doc.ents]
        return {'entities': entities, 'count': len(entities)}

    @staticmethod
    def evaluate_nli(premise: str, hypothesis: str) -> Dict[str, float]:
        try:
            # Fixed: Properly formatting the input for the NLI pipeline
            result = nli_pipeline(
                sequences=hypothesis,
                candidate_labels=["entailment", "contradiction", "neutral"],
            )
            return {
                'label': result['labels'][0],
                'score': result['scores'][0]
            }
        except Exception as e:
            print(f"NLI evaluation error: {e}")
            return {
                'label': 'error',
                'score': 0.0
            }

    @staticmethod
    def evaluate_comprehension(context: str, response: str) -> Dict[str, float]:
        try:
            relevance_score = nlp(context).similarity(nlp(response))
            return {'relevance': float(relevance_score)}
        except Exception as e:
            print(f"Comprehension evaluation error: {e}")
            return {'relevance': 0.0}

    @staticmethod
    def evaluate_topic(text: str) -> Dict[str, str]:
        try:
            # Fixed: Properly formatting the input for topic classification
            result = nli_pipeline(
                sequences=text,
                candidate_labels=["translation", "cultural_analysis", "dialogue", "linguistic"]
            )
            return {
                'predicted_topic': result['labels'][0],
                'confidence': float(result['scores'][0])
            }
        except Exception as e:
            print(f"Topic evaluation error: {e}")
            return {
                'predicted_topic': 'error',
                'confidence': 0.0
            }

    @staticmethod
    def evaluate_translation(source: str, target: str) -> Dict[str, float]:
        try:
            bleu = BLEU()
            bleu_score = bleu.corpus_score([target], [[source]]).score
            return {'bleu_score': float(bleu_score)}
        except Exception as e:
            print(f"Translation evaluation error: {e}")
            return {'bleu_score': 0.0}

def get_gpt4o_response_with_evaluation(prompt: str) -> Dict[str, Any]:
    try:
        # Get GPT-4o response
        response = openai.ChatCompletion.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=2500,
            temperature=0.7
        )
        response_text = response['choices'][0]['message']['content'].strip()

        # Evaluate with error handling for each metric
        evaluations = {}
        try:
            evaluations['dialect_features'] = EvaluationMetrics.evaluate_dialect(response_text)
        except Exception as e:
            print(f"Dialect evaluation error: {e}")
            evaluations['dialect_features'] = {'error': str(e)}

        try:
            evaluations['named_entities'] = EvaluationMetrics.evaluate_ner(response_text)
        except Exception as e:
            print(f"NER evaluation error: {e}")
            evaluations['named_entities'] = {'error': str(e)}

        try:
            evaluations['nli_scores'] = EvaluationMetrics.evaluate_nli(prompt, response_text)
        except Exception as e:
            print(f"NLI evaluation error: {e}")
            evaluations['nli_scores'] = {'error': str(e)}

        try:
            evaluations['comprehension'] = EvaluationMetrics.evaluate_comprehension(prompt, response_text)
        except Exception as e:
            print(f"Comprehension evaluation error: {e}")
            evaluations['comprehension'] = {'error': str(e)}

        try:
            evaluations['topic'] = EvaluationMetrics.evaluate_topic(response_text)
        except Exception as e:
            print(f"Topic evaluation error: {e}")
            evaluations['topic'] = {'error': str(e)}

        try:
            evaluations['translation_quality'] = EvaluationMetrics.evaluate_translation(prompt, response_text)
        except Exception as e:
            print(f"Translation evaluation error: {e}")
            evaluations['translation_quality'] = {'error': str(e)}

        return {
            'response': response_text,
            'evaluations': evaluations
        }
    except Exception as e:
        print(f"Error in GPT-4o response: {e}")
        return {
            'response': str(e),
            'evaluations': {}
        }

def create_bin_prompt(bin_D1, bin_D2, bin_D3):
    prompt = "You are provided with three sets of data entries from Dataset D1, Dataset D2, and Dataset D3:\n\n"

    for idx, (entry_D1, entry_D2, entry_D3) in enumerate(zip(bin_D1, bin_D2, bin_D3), start=1):
        # D1 entries
        prompt += f"**Entry {idx} from Dataset D1**:\n"
        prompt += f"Instruction: {entry_D1.get('Instruction', '')}\n"
        prompt += f"Input: {entry_D1.get('Input', '')}\n"
        prompt += f"Output: {entry_D1.get('Output', '')}\n"
        prompt += f"Tags: {entry_D1.get('Tags', {})}\n\n"

        # D2 entries
        prompt += f"**Entry {idx} from Dataset D2**:\n"
        prompt += f"Context: {entry_D2.get('Context', '')}\n"
        prompt += f"Response: {entry_D2.get('Response', '')}\n"
        prompt += f"ROTS: {entry_D2.get('ROTS', [])}\n"
        prompt += f"Safety Annotations: {entry_D2.get('Safety Annotations', [])}\n"
        prompt += f"Tags: {entry_D2.get('Tags', {})}\n\n"

        # D3 entries
        prompt += f"**Entry {idx} from Dataset D3**:\n"
        prompt += f"Speaker: {entry_D3.get('Speaker', '')}\n"
        prompt += f"Dialogue: {entry_D3.get('Dialogue', '')}\n"
        prompt += f"Tags: {entry_D3.get('Tags', {})}\n\n"

    prompt += """**Task**:
    1. Observe the linguistic features in D2 (Since you are extensively trained in English).\n
    2. Translate the content of each D1 entry into the language or dialect used in its corresponding D2 entry.\n
    3. Conduct a translation task where you translate the dialect in D1 to the dialect/language in D3.\n
    4. Perform a cultural and conversational context analysis of D1 based on the shared similarities with D2 and D3.\n
    5. Detect and report the contexts.\n
    6. Reprompt yourself with the same tasks by using the language spoken in D1, then reprompt yourself with the same tasks by translating the reprompt in D1 with the language spoken in D3.\n
    7. Provide a dialogue response in language D1 (Haitian Creole) or a continuation based on what the speaker said in D2 (English) and D3 (AAVE) respectively.\n
    Note: Carry out this set of tasks once per bin. It is important to analyze the full contents of the bin to provide a proper analysis.\n
    Begin."""

    return prompt

# Process datasets
min_length = min(len(df_D1), len(df_D2), len(df_D3))
bin_size = 3
df_D1 = df_D1.iloc[:min_length]
df_D2 = df_D2.iloc[:min_length]
df_D3 = df_D3.iloc[:min_length]

outputs = []
num_bins = min_length // bin_size

for bin_index in range(num_bins):
    bin_D1 = df_D1.iloc[bin_index * bin_size : (bin_index + 1) * bin_size].to_dict(orient="records")
    bin_D2 = df_D2.iloc[bin_index * bin_size : (bin_index + 1) * bin_size].to_dict(orient="records")
    bin_D3 = df_D3.iloc[bin_index * bin_size : (bin_index + 1) * bin_size].to_dict(orient="records")

    prompt = create_bin_prompt(bin_D1, bin_D2, bin_D3)
    print(f"Processing bin {bin_index + 1}/{num_bins}")

    result = get_gpt4o_response_with_evaluation(prompt)

    outputs.append({
        'Bin': bin_index + 1,
        'Prompt': prompt,
        'Response': result['response'],
        'Evaluations': result['evaluations']
    })

# Create evaluation results
evaluation_results = []
for output in outputs:
    eval_dict = {
        'Bin': output['Bin'],
        'Response': output['Response']
    }

    eval_dict.update({
        'Dialect_Unique_Words': output['Evaluations']['dialect_features']['unique_words'],
        'Dialect_Sentence_Length': output['Evaluations']['dialect_features']['sentence_length'],
        'NER_Entity_Count': output['Evaluations']['named_entities']['count'],
        'NLI_Label': output['Evaluations']['nli_scores']['label'],
        'NLI_Score': output['Evaluations']['nli_scores']['score'],
        'Comprehension_Score': output['Evaluations']['comprehension']['relevance'],
        'Topic_Predicted': output['Evaluations']['topic']['predicted_topic'],
        'Topic_Confidence': output['Evaluations']['topic']['confidence'],
        'Translation_BLEU': output['Evaluations']['translation_quality']['bleu_score']
    })

    for pos, freq in output['Evaluations']['dialect_features']['pos_distribution'].items():
        eval_dict[f'POS_{pos}'] = freq

    evaluation_results.append(eval_dict)

# Create DataFrames
df_outputs = pd.DataFrame(outputs)
df_evaluated = pd.DataFrame(evaluation_results)

print("Original Outputs Sample:")
display(df_outputs.head())
print("\nEvaluation Results Sample:")
display(df_evaluated.head())

# Create summary statistics
print("\nEvaluation Metrics Summary:")
summary_stats = df_evaluated.describe()
display(summary_stats)

# Create visualizations
plt.figure(figsize=(15, 10))

plt.subplot(2, 2, 1)
sns.lineplot(data=df_evaluated, x='Bin', y='Translation_BLEU')
plt.title('BLEU Scores by Bin')

plt.subplot(2, 2, 2)
sns.lineplot(data=df_evaluated, x='Bin', y='Comprehension_Score')
plt.title('Comprehension Scores by Bin')

plt.subplot(2, 2, 3)
sns.lineplot(data=df_evaluated, x='Bin', y='NLI_Score')
plt.title('NLI Scores by Bin')

plt.subplot(2, 2, 4)
sns.lineplot(data=df_evaluated, x='Bin', y='NER_Entity_Count')
plt.title('Named Entity Count by Bin')

plt.tight_layout()
plt.show()



plt.savefig('evaluation_metrics.png')
files.download('evaluation_metrics.png')

# Save results to various formats
df_outputs.to_csv("gpt4o_original_outputs.csv", index=False)
df_evaluated.to_csv("gpt4o_evaluated_outputs.csv", index=False)

with pd.ExcelWriter('gpt4o_complete_analysis.xlsx') as writer:
    df_outputs.to_excel(writer, sheet_name='Original_Outputs', index=False)
    df_evaluated.to_excel(writer, sheet_name='Evaluation_Metrics', index=False)
    summary_stats.to_excel(writer, sheet_name='Summary_Statistics')

# Download all files
files.download('gpt4o_complete_analysis.xlsx')
files.download("gpt4o_original_outputs.csv")
files.download("gpt4o_evaluated_outputs.csv")