\section{Introduction}
\label{sec:introduction}

The use of artificial intelligence in cross-lingual settings has become more prevalent today as Large Language Models (LLMs) continue to take the world by storm. Thanks to their emergence, being able to have natural language support in a cross or multi-lingual is a reality we can enjoy. However, their still remain many under-resourced languages that are not well-supported by LLMs. We intend to further explore this area as the access to LLMs continues to expand beyond developed countries. 

Aimed to improve accessibility and user experience across low-resource languages and dialects, the improvement of translational tasks has become urgent as artificial intelligence incorporates itself into daily life. \cite{Conneau:20} introduced XLM-R, a large-scale multilingual masked language model trained on 100 languages. It demonstrates that pre-training LLMs at scale, can lead to significant performance gains for a wide range of cross-lingual transfer tasks, yet we haven't quite explored low-resource dialects and obscure languages that are not so globally spoken. Haitian Creole, to our knowledge is one such language that remains understudied by researchers working with LLMs.

Our study contributes to this expanding field by focusing on LLM performance in Haitian Creole cross-lingual tasks. Specifically leveraging datasets of African American Vernacular English (AAVE) and Standard American English as our poly-glottal and high-resource languages. 

We present a novel methodology to improve the generative and analytical capabilities of LLMs in a cross-lingual setting between SAE, AAVE and Haitian Creole using transformer-based models. Our approach centers on observing pre-trained multilingual transformer model performance with a combination dialect-specific datasets and techniques to see where trade-offs and improvements are made depending on what is combined. 

By hybridizing state of the art techniques, Linguistically-Diverse Prompting (LDP) \cite{Nguyen:24}, Polyglot Prompting \cite{Ng:22} which focus respectively on combining high-resource to low-resource languages and low-resource to low-resource languages, and contextual tags we investigate the improvement of adapting language models to Haitian Creole \cite{Upadhayay2024}. The integration of these techniques allows us to address the unique challenges presented by low-resource languages while maintaining sensitivity to their cultural and linguistic characteristics. By leveraging this methodology we show that Transformer models can effectively capture the subtle linguistic nuances that distinguish these dialects from high resource languages thus performing better in a cross-lingual setting. By enhancing the accuracy of multi-lingual identification and translation we promote greater social inclusion in language processing technologies and Artificial Intelligence in developing countries. 


