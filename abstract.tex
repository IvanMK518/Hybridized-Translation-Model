\begin{abstract}
Low-resource languages and dialects such as Haitian Creole and African American Vernacular English (AAVE) often face significant challenges in natural language processing due to limited training data and complex linguistic variations. This paper introduces a novel approach combining Linguistically-Diverse Prompting (LDP), Polyglot Prompting, and conversational context tagging techniques to improve model performance across these linguistic variants. Through experiments with GPT-4o, we evaluate four model configurations that employ a combination of datasets and given techniques. Our results reveal interesting trade-offs: while simpler models excel at Natural Language Inference tasks, our more complex hybrid model demonstrates superior performance in Machine Reading Comprehension and Machine Translation. By incorporating contextual markers and poly-glottal datasets, our approach surpasses the base model's ability to handle dialectal variations while maintaining cultural sensitivity in dialogue responses. 
\end{abstract}