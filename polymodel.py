# -*- coding: utf-8 -*-
"""PolyModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r6u5eh35qgKlpYn1Wl0nWvYXL0qkiKcj
"""

# Cell 1: Initial Setup and Installations
!pip install openai
!pip install gdown
!pip uninstall -y openai
!pip install openai==0.28.0
!pip install spacy transformers sacrebleu bert-score torch
!python -m spacy download en_core_web_lg

# Cell 2: Imports and Initializations
import openai
import gdown
import json
import pandas as pd
import spacy
from transformers import pipeline
from sacrebleu.metrics import BLEU
from bert_score import score as bert_score
import numpy as np
from typing import List, Dict, Any
import time
from json.decoder import JSONDecodeError

# Initialize NLP components
nlp = spacy.load("en_core_web_lg")
nli_pipeline = pipeline("zero-shot-classification")
qa_pipeline = pipeline("question-answering")

# API key
openai.api_key = "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"

# Cell 3: Evaluation Metrics Class
class EvaluationMetrics:
    @staticmethod
    def evaluate_dialect(text: str) -> Dict[str, float]:
        doc = nlp(text)
        features = {
            'sentence_length': len(text.split()),
            'unique_words': len(set(text.lower().split())),
            'pos_distribution': dict(pd.Series([token.pos_ for token in doc]).value_counts(normalize=True))
        }
        return features

    @staticmethod
    def evaluate_ner(text: str) -> Dict[str, List[Dict]]:
        doc = nlp(text)
        entities = [{'text': ent.text, 'label': ent.label_} for ent in doc.ents]
        return {'entities': entities, 'count': len(entities)}

    @staticmethod
    def evaluate_nli(premise: str, hypothesis: str) -> Dict[str, float]:
        try:
            result = nli_pipeline(
                sequences=hypothesis,
                candidate_labels=["entailment", "contradiction", "neutral"],
            )
            return {
                'label': result['labels'][0],
                'score': result['scores'][0]
            }
        except Exception as e:
            print(f"NLI evaluation error: {e}")
            return {
                'label': 'error',
                'score': 0.0
            }

    @staticmethod
    def evaluate_comprehension(context: str, response: str) -> Dict[str, float]:
        try:
            relevance_score = nlp(context).similarity(nlp(response))
            return {'relevance': float(relevance_score)}
        except Exception as e:
            print(f"Comprehension evaluation error: {e}")
            return {'relevance': 0.0}

    @staticmethod
    def evaluate_topic(text: str) -> Dict[str, str]:
        try:
            result = nli_pipeline(
                sequences=text,
                candidate_labels=["translation", "cultural_analysis", "dialogue", "linguistic"]
            )
            return {
                'predicted_topic': result['labels'][0],
                'confidence': float(result['scores'][0])
            }
        except Exception as e:
            print(f"Topic evaluation error: {e}")
            return {
                'predicted_topic': 'error',
                'confidence': 0.0
            }

    @staticmethod
    def evaluate_translation(source: str, target: str) -> Dict[str, float]:
        try:
            bleu = BLEU()
            bleu_score = bleu.corpus_score([target], [[source]]).score
            return {'bleu_score': float(bleu_score)}
        except Exception as e:
            print(f"Translation evaluation error: {e}")
            return {'bleu_score': 0.0}

# Cell 4: Helper Functions
def download_from_drive(file_id, output_name):
    gdown.download(f"https://drive.google.com/uc?id={file_id}", output_name, quiet=False)

def get_gpt4o_response(prompt: str) -> Dict[str, Any]:
    try:
        response = openai.ChatCompletion.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=2500,
            temperature=0.7
        )
        response_text = response['choices'][0]['message']['content'].strip()

        # Add evaluations
        evaluations = {}
        try:
            evaluations['dialect_features'] = EvaluationMetrics.evaluate_dialect(response_text)
            evaluations['named_entities'] = EvaluationMetrics.evaluate_ner(response_text)
            evaluations['nli_scores'] = EvaluationMetrics.evaluate_nli(prompt, response_text)
            evaluations['comprehension'] = EvaluationMetrics.evaluate_comprehension(prompt, response_text)
            evaluations['topic'] = EvaluationMetrics.evaluate_topic(response_text)
            evaluations['translation_quality'] = EvaluationMetrics.evaluate_translation(prompt, response_text)
        except Exception as e:
            print(f"Evaluation error: {e}")
            evaluations['error'] = str(e)

        return {
            'response': response_text,
            'evaluations': evaluations
        }
    except Exception as e:
        print(f"GPT-4o response error: {e}")
        return {'error': str(e)}

def create_bin_prompt(bin_D1, bin_D3):
    prompt = "You are provided with two sets of data entries from Dataset D1 and Dataset D3:\n\n"

    for idx, (entry_D1, entry_D3) in enumerate(zip(bin_D1, bin_D3), start=1):
        # Extract fields from Dataset D1
        instruction = entry_D1.get('Instruction', '')
        input_text = entry_D1.get('Input', '')
        output_text = entry_D1.get('Output', '')
        tags_D1 = entry_D1.get('Tags', {})

        # Extract fields from Dataset D3
        speaker = entry_D3.get('Speaker', '')
        dialogue = entry_D3.get('Dialogue', '')
        tags_D3 = entry_D3.get('Tags', {})

        prompt += f"**Entry {idx} from Dataset D1**:\nInstruction: {instruction}\nInput: {input_text}\nOutput: {output_text}\nTags: {tags_D1}\n\n"
        prompt += f"**Entry {idx} from Dataset D3**:\nSpeaker: {speaker}\nDialogue: {dialogue}\nTags: {tags_D3}\n\n"

    prompt += """**Task**:
    1. Conduct a translation task where you translate the dialect in D1 to the dialect/language in D3.
    2. Perform a cultural and conversational context analysis of D1 based on the shared tags with D3.
    3. Detect and report the contexts.
    4. Provide a dialogue response in language D1 or a continuation based on what the speaker said in D3.
    5. Reprompt yourself with the same tasks by using the language spoken in D1.

    Note: Carry out this set of tasks once per bin. It is important to analyze the full contents of the bin to provide a proper analysis.
    Begin."""

    return prompt

# Cell 5: Data Loading and Processing
# Download D1 and D3
download_from_drive("10P4Vvombs_lsFjTwEO5rskpEu6RP8MnL", "D1_dataset.json")
download_from_drive("10A56TDD5V4kh9C-oKB75F4_2TdLp4yq-", "D3_dataset.json")

# Load datasets
with open("D1_dataset.json", "r", encoding="utf-8") as file:
    data_D1 = json.load(file)
with open("D3_dataset.json", "r", encoding="utf-8") as file:
    data_D3 = json.load(file)

# Convert to DataFrames
df_D1 = pd.json_normalize(data_D1)
df_D3 = pd.json_normalize(data_D3)

# Preview datasets
print("D1 Sample:")
display(df_D1.head())
print("D3 Sample:")
display(df_D3.head())

# Ensure same length
min_length = min(len(df_D1), len(df_D3))
df_D1 = df_D1.iloc[:min_length]
df_D3 = df_D3.iloc[:min_length]

# Cell 6: Main Processing Loop
outputs = []
bin_size = 5
num_bins = min_length // bin_size

for bin_index in range(num_bins):
    # Select the entries for the current bin
    bin_D1 = df_D1.iloc[bin_index * bin_size : (bin_index + 1) * bin_size].to_dict(orient="records")
    bin_D3 = df_D3.iloc[bin_index * bin_size : (bin_index + 1) * bin_size].to_dict(orient="records")

    # Create prompt for the current bin
    prompt = create_bin_prompt(bin_D1, bin_D3)
    print(f"Processing bin {bin_index + 1}/{num_bins}")

    # Get GPT-4o response with evaluations
    result = get_gpt4o_response(prompt)

    # Store the result
    outputs.append({
        'Bin': bin_index + 1,
        'Prompt': prompt,
        'Response': result['response'],
        'Evaluations': result.get('evaluations', {})
    })

# Cell 7: Save and Display Results
# Create DataFrame with outputs
df_outputs = pd.DataFrame(outputs)

# Save to CSV
df_outputs.to_csv("gpt4o_polyglot_evaluated_outputs.csv", index=False)

# Create evaluation summary
print("\nEvaluation Metrics Summary:")
evaluation_metrics = pd.json_normalize(df_outputs['Evaluations'].tolist())
display(evaluation_metrics.describe())

# Download results
from google.colab import files
files.download("gpt4o_polyglot_evaluated_outputs.csv")

# Create visualizations
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(15, 10))

plt.subplot(2, 2, 1)
sns.lineplot(data=evaluation_metrics, x=evaluation_metrics.index, y='translation_quality.bleu_score')
plt.title('BLEU Scores by Entry')

plt.subplot(2, 2, 2)
sns.lineplot(data=evaluation_metrics, x=evaluation_metrics.index, y='comprehension.relevance')
plt.title('Comprehension Scores by Entry')

plt.subplot(2, 2, 3)
sns.lineplot(data=evaluation_metrics, x=evaluation_metrics.index, y='nli_scores.score')
plt.title('NLI Scores by Entry')

plt.subplot(2, 2, 4)
sns.lineplot(data=evaluation_metrics, x=evaluation_metrics.index, y='named_entities.count')
plt.title('Named Entity Count by Entry')

plt.tight_layout()
plt.savefig('evaluation_metrics.png')
files.download('evaluation_metrics.png')